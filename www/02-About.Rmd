---
title: "About"
author: "Gabriel Muñoz"
date: "7/28/2017"
output: html_document
---


## Why mining data from literature

To better understand large-scale process in Ecology it is necessary to develop new hypothesis based on our current knowledge.  Global biodiversity research depends on [high - level](https://en.wikipedia.org/wiki/Biological_organisation) concepts abstracted and put together to synthesize a variety of information independently produced by local observers, teams, and institutions working differently all over the world (Bisby 2000). This array of sources creates an intrinsic difficulty on "knowing what is where" and "comparing like with like" (Bisby 2000), fueling the need for integration of different ecological data and information over various geographical and environmental scales (Thuiller et al. 2013; Kissling and Schleuning 2015; Poissot et al. 2016).

A good portion of primary biodiversity knowledge on species is found scattered amongst all the academic and non-academic works available through the internet as PDF articles. Searching for specific biodiversity data among all this information means also sorting out many unrelated articles. This makes the efforts of summarizing biodiversity knowledge from literature manual and time consuming for a researcher. Using text mining algorithms, this shiny app optimizes the search for biodiversity data present in scientific articles/gray literature.

----------


# Where to harvest articles? 

There is many ways to get articles. If you have the access to Academic Databases ( e.g. [Web of Science](https://login.webofknowledge.com/)) you can just download as many articles you want and feed them to the app. However, there a known publication bias on harvesting references from only one database. Therefore, below you can find a list of sources to search for academic publications. Ideally, to avoid such biases, specially if your focusing your research on tropical regions or zones where english is not the main lenguage, a good literature search should involves queries in as many databases possible.

+ [arXiv](arxiv.org) (Free) 
+ [Redalyc](www.redalyc.org) (Free) 
+ [FreeFullPDF](www.freefullpdf.com) (Free) 
+ [Wiley](www.wiley.com) (Subscription needed) 
+ [CORE](core.ac.uk) (Free) 
+ [Microsoft Academic](academic.microsoft.com) (Free for 10K queries per month) 
+ [Springer](www.springer.com) (Subscription needed) 
+ [Web of Science](webofknowledge.com) (Subscription needed) 
+ [Russian Science Citation Index](elibrary.ru) (Free) 
+ [SciELO](www.scielo.org) (Free) 
+ [Directory of Open Access Journals](www.doaj.org) (Free) 
+ [BioOne](www.bioonepublishing.org) (Subscription needed) 
+ [Google Scholar](scholar.google.com)
+ [ScienceOpen](scienceopen.com) (Free) 
+ [Scopus](www.scopus.com) (Free) 
+ [Socolar](www.socolar.com) (Free) 
+ [Public Library of Science](www.plos.org)
+ [EuropePMC](europepmc.org) (Free) 
+ [Crossref](www.crosref.org) (Free) 
+ [Biblat](biblat.unam.mx) (Free)  
+ [Sci-Hub](sci-hub.cc) (Free) 

----------



## Programatic query and download 

It is possible to (semi)- automatize the process of downloading articles. If a researcher is interested into programatically query Google Scholar, there is an unofficial API [sci-hub.py](https://github.com/zaytoun/scihub.py). This API search for articles in Google Scholar and download them from Sci-hub when there is not a free version available. This API only works for python and currently the automatic download from Sci - Hub is blocked by captchas.  However, the list of urls directing to the articles from a particular query in Google scholar can be retrieved fairly easy. After doing some preprocessing on this list the urls can be passed to the `download.file` implemented in base R for download the PDFs. 

If you are not so familiar with programming, another very good option to shorten the process of downloading articles is described in [Haddaway et al. 2017](https://environmentalevidencejournal.biomedcentral.com/articles/10.1186/s13750-016-0079-2). 

----------

# How to create a thesaurus? 

To identify custom relevant ecological information you can create your own dictionary or thesaurus. A thesaurus is a list of common "standard" terms that researchers use to describe a particular process in ecology.  For example, to describe frugivory events the words `diet`, `frugivore`, `seed dispersal` are used frequently. To build the thesaurus you could simply list your common terms of interest. Although the pool of terms used to describe a particular ecological event is finite, it can become difficult (even for an specialist) to recall all terms, therefore it is a better option to extract those terms directly from small collection of "key" articles ( articles you know that contain the desired information). Building a thesaurus of terms becomes easier if you have some "key" articles already identified. Web tools, such as  (add references to TERMMMINE, etc...), exist to extract the main terms of a document or a collection of documents. If your familiar with R, packages like `tm` to extract terms from [corpus](reference to corpus) of articles, calculate term frequencies, etc. 

----------

# How does it work? 

Ecological Events Miner makes use of R packages designed for text mining and base R functions. 
Functions from `fulltext` are used to perform the OCR. The `taxize` package is used to establish the API connection to the [Global Names Recognition and Discovery (GNRF)](rdrr.io) tool. The `stringr`is used for string manipulation and `wordcloud` to create the wordcloud of terms. 

## Required packages to fuction

- shinythemes
- DT
- stringi
- stringr
- wordcloud
- taxize
- fulltext

----------

# I have mined my data, now what? 


Well, once you have mined your data, test your new hyphotesis and discover new trends. Please share the data and make it available to others. [Zenodo](https://zenodo.org/) is a good free repository for any kind of data. More specific repositories also exist for: 
  
  + Species traits data:  TRY
  + Species interaction data: GLOBI
  + Species occurrence data: GBIF
  
By minding the gap between the availability of data from published literature to digitally accesible you will contribute to increase the general global understanding of key ecological processes and to identify specific and geographical gaps in our shared ecological knowledge. 

----------

# Room for improvement? lets get in touch. 

There is always room for improvement, for now (Because of time and knowledge limitations) I still haven't implemented all my possible ideas for this app. Ecolgical Event Miner is still a work in progress so if you have recomendations, suggestions or are experiencing any issues send me a message I'll be happy to discuss new ideas and tools for improvement!

Contact: [email](mailto:fgabriel1891@gmail.com), find me on [GitHub](https://github.com/fgabriel1891) or [ResearchGate](https://www.researchgate.net/profile/Gabriel_Munoz2).

----------

# References and Further reading



Bisby, F. A. (2000). The quiet revolution: biodiversity informatics and the internet. Science, 289(5488), 2309-2312.

Haddaway, N. R., Collins, A. M., Coughlin, D., & Kirk, S. (2017). A rapid method to increase transparency and efficiency in web-based searches. Environmental Evidence, 6(1), 1.

Kissling, W. D., & Schleuning, M. (2015). Multispecies interactions across trophic levels at macroscales: retrospective and future directions. Ecography, 38(4), 346-357.

Poisot, T., Stouffer, D. B., & Gravel, D. (2015). Beyond species: why ecological interaction networks vary through space and time. Oikos, 124(3), 243-251.

Thuiller, W., Münkemüller, T., Lavergne, S., Mouillot, D., Mouquet, N., Schiffers, K., & Gravel, D. (2013). A road map for integrating eco‐evolutionary processes into biodiversity models. Ecology letters, 16(s1), 94-105.

----------

# Aknowledgements 

For helpful discussions and commentaries I am grateful to both [Emiel van Loon](https://staff.science.uva.nl/e.e.vanloon/index.html) and [Daniel Kissling](http://www.uva.nl/en/profile/k/i/w.d.kissling/w.d.kissling.html) from the University of Amsterdam. 

Ecological Event Miner uses tools from the [GNA (GlobalNamesArchitecture)](https://github.com/GlobalNamesArchitecture) implemented in the [taxize](https://github.com/ropensci/taxize) package. Thanks to Scott Chamberlain for modifications to ´taxize´ that improves the functionality of this app.
Credits to the developers of the individual packages Ecological Event Miner is dependent on.  
